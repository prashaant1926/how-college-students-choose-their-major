# AI-Augmented Academic Decision Making

## Key Papers

### AI-Augmented Advising  
**Authors**: Kasra Lekan, Zachary A. Pardos (2025)  
**Source**: Journal of Learning Analytics  
**URL**: https://learning-analytics.info/index.php/JLA/article/view/8593

#### Analysis Framework

**Problem**: Can large language models (LLMs) like GPT-4 effectively augment human academic advising for major selection?

**Prior Assumptions**: Information Sufficiency Assumption - providing more or better information about majors and career outcomes improves decision quality.

**Insight**: AI can provide highly helpful major recommendations that advisors rate favorably (4.0/5 for explanations, 3.8/5 for answers), with 33% agreement rate between AI and human advisors.

**Technical Approach**: Three-phase survey comparing GPT-4 suggestions with expert human advisor responses for undeclared first- and second-year students (n=33 students, n=25 advisors).

**Evaluation**: Used Likert scale ratings from advisors, agreement metrics between AI and human recommendations, and qualitative feedback categorization.

**Impact**: Partially challenges Information Sufficiency Assumption - while AI provides helpful information, agreement rates suggest information alone may not be sufficient, and human expertise remains critical for personalized guidance.

---

### Choosing Between an LLM versus Search for Learning: A Higher Ed Student Perspective
**Authors**: Rahul R. Divekar (2024)  
**Source**: arXiv:2409.13051  
**URL**: https://arxiv.org/html/2409.13051v1

#### Analysis Framework

**Problem**: How do students choose between LLMs (like ChatGPT) versus traditional search engines (like Google) for learning and academic help-seeking?

**Prior Assumptions**: Rationality Assumption - students systematically evaluate tools based on cost-benefit analysis for learning outcomes.

**Insight**: Students increasingly prefer ChatGPT for academic assistance, but this preference is driven more by convenience and immediate satisfaction than systematic evaluation of learning effectiveness.

**Technical Approach**: Within-subjects counterbalanced design where students used both search engines and LLMs for learning new topics, followed by post-experience interviews.

**Evaluation**: Analyzed student reflections, preferences, and pain points through qualitative analysis of interview data.

**Impact**: Challenges Rationality Assumption - tool choice appears more driven by experiential factors (ease of use, immediate gratification) than deliberate analysis of learning outcomes.

---

### Google or ChatGPT: Who is the Better Helper for University Students
**Authors**: Xiantong Yang et al. (2024)  
**Source**: arXiv:2405.00341  
**URL**: https://arxiv.org/abs/2405.00341

#### Analysis Framework

**Problem**: What factors influence university students' preferences for using ChatGPT versus Google for academic help-seeking?

**Prior Assumptions**: Future Orientation Assumption - students choose tools based on projected academic outcomes and long-term learning benefits.

**Insight**: GenAI fluency, GenAI distortions, and age are core factors influencing academic help-seeking tool choice. Students prefer ChatGPT despite potential accuracy concerns.

**Technical Approach**: Mixed-methods research with Taiwanese university students, comparing seven machine learning algorithms (Random Forest and LightGBM performed best) to predict tool preference using 18 potential factors.

**Evaluation**: Used machine learning models to identify predictive factors, with superior performance from Random Forest and LightGBM algorithms.

**Impact**: Challenges Future Orientation Assumption - immediate usability factors (fluency, convenience) predict tool choice better than long-term learning considerations. Supports Present-Moment Decision Making hypothesis.